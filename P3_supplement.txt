
######################
# Supplemental Questions #
######################


Answer the supplemental questions here! Make sure you follow the format if it is asked
Q1#######################
QS1.1: For computeActionFromValues(state), I interpreted it as a component of the value iteration function given at the beginning of question 1. In particular, I interpreted it as the max of the function, which finds the action with the highest Q-value out of all possible actions for a given state. With this in mind, my implementation gets all possible actions from the state (disregarding terminal states) before using Python's 'max' function to find the action with the highest Q-value. These Q-values are taken from the results of calling our other implemented method (computeQValueFromValues) and applying those results exclusively within the context of this method call using lambda. The best (or highest Q-value) action from these actions is chosen as the return value given to the calling function.

QS1.2: As for computeQValueFromValues(state, action), this method is the other component of the value iteration function used in question 1, that being the calculation and aggregation of the values of all possible outcomes for an action in a given state. My implementation starts with initializing a Q-value before iterating through all possible outcomes of taking the specified action, adding the product of each outcome's value and possibility of occurring to the final Q-value for the action. When this process is complete, the final aggregated Q-value of taking the action is returned to the calling function.

Q3#######################

QS3.1: For each policy of question 3, I assigned each corresponding function in analysis.py appropriate values for the discount, noise, and living reward of the policy.

For policy 1 (Prefer the close exit (+1), risking the cliff (-10)), I chose the respective values of 0.9, 0.0, and -4.0. I chose these values because they incentivize pursuing the closest exit possible without regard for the risk of other possible transitions.

For policy 2 (Prefer the close exit (+1), but avoiding the cliff (-10)), I chose the respective values of 0.4, 0.2, and -3.0. I chose these values because, while they heavily incentize acting rashly to reach the closest action ASAP, they still provide some consideration of risky moves that ultimately steers the agent away from cliffs.

For policy 3 (Prefer the distant exit (+10), risking the cliff (-10)), I chose the respective values of 0.9, 0.0, and 0.0. I chose these values because they incentivize taking a bit more time to reach farther exits without much regard for risky moves while still providing some small pressure to keep moving.

For policy 4 (Prefer the distant exit (+10), avoiding the cliff (-10)), I chose the respective values of 0.9, 0.2, and 0.0. I chose these values because they perform similarly to policy 4 while incorporating some consideration of risky moves that disincentivizes staying near cliffs.

For policy 5 (Avoid both exits and the cliff (so an episode should never terminate)), I chose the respective values of 0.0, 0.2, and 1.0. I chose these values because there should be no incentive for pursuing exit rewards, and instead there should be a positive incentive for staying alive as long as possible while still disincentivizing staying near cliffs.

Q5#######################

QS5.1: My implementation is pretty simple, computeQValueFromValues is just getting the max from the list of actions using a for loop. computeActionFromValues just iterates over the list of actions and their values then keeps a running list of the best actions to take depending on their value. Best value is always updated, and if a new best value is determined, the old best value and action is deleted from the list. If we have a tie, the action gets added then we randomly select one of the best actions to send to the agent.

QS5.2 [optional]: https://github.com/camjaycecamp/CS4013-AI-Project3/blob/main/Screenshot%202024-03-16%20102809.png
The main differences you can see is with the ending policy you recieve. I ran it a couple of times and got all sorts of variations. With noise I can see a world that can lead to a better answer than the one you currently have faster, but it can also lead to a worse ending result as sometimes you just get really bad luck on the paths that the agent will take.
Q6#######################
QS6.1: The implementation on this is also pretty simple. We flip a coin who's value is determined by epsilon, which is always under 1 meaning we can just use that as our boolean value. Depending on the outcome of util.flipCoin(p) we either take a random potentially suboptimal path in the legalActions list, or we compute the optimal action from our computeActionFromValues() function from earlier.
QS6.2 [optional]: https://github.com/camjaycecamp/CS4013-AI-Project3/blob/main/Screenshot%202024-03-16%20101624.png


Q7#######################
QS7.1 After testing it with different values, high and low, equal, and all sorts, I don't believe it is possible. There has to be a lot of randomness at the start for the agent to be able to explore very
quickly and try to find where the rewards are, but then it needs to learn and narrow down its search equally as fast. I could not find any values that satisfied this enough. Perhaps if you changed epsilon to be
able to be variable depending on how long you've stayed alive, then maybe, but for our case I just don't see any two values that work consistently.




