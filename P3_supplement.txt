
######################
# Supplemental Questions #
######################


Answer the supplemental questions here! Make sure you follow the format if it is asked
Q1#######################
QS1.1: For computeActionFromValues(state), I interpreted it as a component of the value iteration function given at the beginning of question 1. In particular, I interpreted it as the max of the function, which finds the action with the highest Q-value out of all possible actions for a given state. With this in mind, my implementation gets all possible actions from the state (disregarding terminal states) before using Python's 'max' function to find the action with the highest Q-value. These Q-values are taken from the results of calling our other implemented method (computeQValueFromValues) and applying those results exclusively within the context of this method call using lambda. The best (or highest Q-value) action from these actions is chosen as the return value given to the calling function.

QS1.2: As for computeQValueFromValues(state, action), this method is the other component of the value iteration function used in question 1, that being the calculation and aggregation of the values of all possible outcomes for an action in a given state. My implementation starts with initializing a Q-value before iterating through all possible outcomes of taking the specified action, adding the product of each outcome's value and possibility of occurring to the final Q-value for the action. When this process is complete, the final aggregated Q-value of taking the action is returned to the calling function.

Q3#######################

QS3.1: For each policy of question 3, I assigned each corresponding function in analysis.py appropriate values for the discount, noise, and living reward of the policy.

For policy 1 (Prefer the close exit (+1), risking the cliff (-10)), I chose the respective values of 0.9, 0.0, and -4.0. I chose these values because they incentivize pursuing the closest exit possible without regard for the risk of other possible transitions.

For policy 2 (Prefer the close exit (+1), but avoiding the cliff (-10)), I chose the respective values of 0.4, 0.2, and -3.0. I chose these values because, while they heavily incentize acting rashly to reach the closest action ASAP, they still provide some consideration of risky moves that ultimately steers the agent away from cliffs.

For policy 3 (Prefer the distant exit (+10), risking the cliff (-10)), I chose the respective values of 0.9, 0.0, and 0.0. I chose these values because they incentivize taking a bit more time to reach farther exits without much regard for risky moves while still providing some small pressure to keep moving.

For policy 4 (Prefer the distant exit (+10), avoiding the cliff (-10)), I chose the respective values of 0.9, 0.2, and 0.0. I chose these values because they perform similarly to policy 4 while incorporating some consideration of risky moves that disincentivizes staying near cliffs.

For policy 5 (Avoid both exits and the cliff (so an episode should never terminate)), I chose the respective values of 0.0, 0.2, and 1.0. I chose these values because there should be no incentive for pursuing exit rewards, and instead there should be a positive incentive for staying alive as long as possible while still disincentivizing staying near cliffs.

Q5#######################

QS5.1:

QS5.2 [optional]:

Q6#######################
QS6.1:
QS6.2 [optional]:


Q7#######################
QS7.1




